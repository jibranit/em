{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Along :: Feature Selection and Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"spam\" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography... \n",
    "\n",
    "Our collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word 'george' and the area code '650' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter. \n",
    "\n",
    "-  Number of Instances: 4601 (1813 Spam = 39.4%)\n",
    "-  Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
    "\n",
    " -  Attribute Information:\n",
    "\n",
    "    -  The last column of 'spambase.data' denotes whether the e-mail was \n",
    "       considered spam (1) or not (0)\n",
    "    \n",
    "    - 48 attributes are continuous real [0,100] numbers of type `word freq WORD` i.e. percentage of words in the e-mail that         match WORD\n",
    "\n",
    "    - 6 attributes are continuous real [0,100] numbers of type `char freq CHAR` i.e. percentage of characters in the e-mail           that match CHAR\n",
    "    \n",
    "    - 1 attribute is continuous real [1,...] numbers of type `capital run length average` i.e. average length of uninterrupted       sequences of capital letters\n",
    "\n",
    "    - 1 attribute is continuous integer [1,...] numbers of type `capital run length longest` i.e. length of longest                   uninterrupted sequence of capital letters\n",
    "\n",
    "    - 1 attribute is continuous integer [1,...] numbers of type `capital run length total` i.e. sum of length of uninterrupted       sequences of capital letters in the email\n",
    "\n",
    "    - 1 attribute is nominal {0,1} class  of type spam i.e  denotes whether the e-mail was considered spam (1) or not (0),  \n",
    "\n",
    "- Missing Attribute Values: None\n",
    "\n",
    "- Class Distribution:\n",
    "\tSpam\t  1813  (39.4%)\n",
    "\tNon-Spam  2788  (60.6%)\n",
    "\n",
    "\n",
    "\n",
    "You can read more about dataset [here](https://archive.ics.uci.edu/ml/datasets/spambase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9  ...    48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00 ...  0.00  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00  0.132   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01  0.143   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.137   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.135   \n",
       "\n",
       "    50     51     52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the Spam data for the mini challenge\n",
    "#Target variable is the 57 column i.e spam, non-spam classes \n",
    "df = pd.read_csv('spambase.data.csv',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get an overview of your data by using info() and describe() functions of pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4601 entries, 0 to 4600\n",
      "Data columns (total 58 columns):\n",
      "0     4601 non-null float64\n",
      "1     4601 non-null float64\n",
      "2     4601 non-null float64\n",
      "3     4601 non-null float64\n",
      "4     4601 non-null float64\n",
      "5     4601 non-null float64\n",
      "6     4601 non-null float64\n",
      "7     4601 non-null float64\n",
      "8     4601 non-null float64\n",
      "9     4601 non-null float64\n",
      "10    4601 non-null float64\n",
      "11    4601 non-null float64\n",
      "12    4601 non-null float64\n",
      "13    4601 non-null float64\n",
      "14    4601 non-null float64\n",
      "15    4601 non-null float64\n",
      "16    4601 non-null float64\n",
      "17    4601 non-null float64\n",
      "18    4601 non-null float64\n",
      "19    4601 non-null float64\n",
      "20    4601 non-null float64\n",
      "21    4601 non-null float64\n",
      "22    4601 non-null float64\n",
      "23    4601 non-null float64\n",
      "24    4601 non-null float64\n",
      "25    4601 non-null float64\n",
      "26    4601 non-null float64\n",
      "27    4601 non-null float64\n",
      "28    4601 non-null float64\n",
      "29    4601 non-null float64\n",
      "30    4601 non-null float64\n",
      "31    4601 non-null float64\n",
      "32    4601 non-null float64\n",
      "33    4601 non-null float64\n",
      "34    4601 non-null float64\n",
      "35    4601 non-null float64\n",
      "36    4601 non-null float64\n",
      "37    4601 non-null float64\n",
      "38    4601 non-null float64\n",
      "39    4601 non-null float64\n",
      "40    4601 non-null float64\n",
      "41    4601 non-null float64\n",
      "42    4601 non-null float64\n",
      "43    4601 non-null float64\n",
      "44    4601 non-null float64\n",
      "45    4601 non-null float64\n",
      "46    4601 non-null float64\n",
      "47    4601 non-null float64\n",
      "48    4601 non-null float64\n",
      "49    4601 non-null float64\n",
      "50    4601 non-null float64\n",
      "51    4601 non-null float64\n",
      "52    4601 non-null float64\n",
      "53    4601 non-null float64\n",
      "54    4601 non-null float64\n",
      "55    4601 non-null int64\n",
      "56    4601 non-null int64\n",
      "57    4601 non-null int64\n",
      "dtypes: float64(55), int64(3)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Overview of the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.104553     0.213015     0.280656     0.065425     0.312223   \n",
       "std       0.305358     1.290575     0.504143     1.395151     0.672513   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.420000     0.000000     0.380000   \n",
       "max       4.540000    14.280000     5.100000    42.810000    10.000000   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.095901     0.114208     0.105295     0.090067     0.239413   \n",
       "std       0.273824     0.391441     0.401071     0.278616     0.644755   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.160000   \n",
       "max       5.880000     7.270000    11.110000     5.260000    18.180000   \n",
       "\n",
       "          ...                48           49           50           51  \\\n",
       "count     ...       4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      ...          0.038575     0.139030     0.016976     0.269071   \n",
       "std       ...          0.243471     0.270355     0.109394     0.815672   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.065000     0.000000     0.000000   \n",
       "75%       ...          0.000000     0.188000     0.000000     0.315000   \n",
       "max       ...          4.385000     9.752000     4.081000    32.478000   \n",
       "\n",
       "                52           53           54           55            56  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   4601.000000   \n",
       "mean      0.075811     0.044238     5.191515    52.172789    283.289285   \n",
       "std       0.245882     0.429342    31.729449   194.891310    606.347851   \n",
       "min       0.000000     0.000000     1.000000     1.000000      1.000000   \n",
       "25%       0.000000     0.000000     1.588000     6.000000     35.000000   \n",
       "50%       0.000000     0.000000     2.276000    15.000000     95.000000   \n",
       "75%       0.052000     0.000000     3.706000    43.000000    266.000000   \n",
       "max       6.003000    19.829000  1102.500000  9989.000000  15841.000000   \n",
       "\n",
       "                57  \n",
       "count  4601.000000  \n",
       "mean      0.394045  \n",
       "std       0.488698  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data into train and test set and fit the base logistic regression model on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the dataset set in train and test set and apply base logistic model\n",
    "X = df.drop([57],1)\n",
    "y =df[57]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find out the accuracy , print out the Classification report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       804\n",
      "           1       0.94      0.90      0.92       577\n",
      "\n",
      "    accuracy                           0.93      1381\n",
      "   macro avg       0.93      0.93      0.93      1381\n",
      "weighted avg       0.93      0.93      0.93      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9319333816075308\n",
      "f1_score is 0.9166666666666667\n",
      "A precision_score is 0.9382940108892922\n",
      "A recall_score is 0.8960138648180243\n",
      "A roc_auc_score is 0.926862653802047\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Classification report and Confusion Matrix.\n",
    "from sklearn.metrics import precision_score,recall_score, roc_auc_score,accuracy_score ,f1_score\n",
    "print('Accuracy Score is',accuracy_score(y_test,y_pred))\n",
    "print('f1_score is',f1_score(y_test,y_pred))\n",
    "print('A precision_score is',precision_score(y_test,y_pred))\n",
    "print('A recall_score is',recall_score(y_test,y_pred))\n",
    "print('A roc_auc_score is',roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517 34\n",
      "60 770\n"
     ]
    }
   ],
   "source": [
    "tn,fp,fn,tp = confusion_matrix(y_test,y_pred).ravel()\n",
    "print(tp,fp)\n",
    "print(fn,tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Copy dataset df into df1 variable and apply correlation on df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy df in new variable df1\n",
    "df1 = df.copy()\n",
    "correlation = X.corr().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. As we have learned  one of the assumptions of Logistic Regression model is that the independent features should not be correlated to each other(i.e Multicollinearity), So we have to find the features that have a correlation higher that 0.75 and remove the same so that the assumption for logistic regression model is satisfied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31  33    0.996066\n",
       "    39    0.848021\n",
       "33  31    0.996066\n",
       "    39    0.845359\n",
       "39  31    0.848021\n",
       "    33    0.845359\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Correlated features above 0.75 and then apply logistic model\n",
    "#correlation1 = X#.sort_values(kind='quicksort')\n",
    "correlation[(abs(correlation)>0.75) & (abs(correlation) != 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_var_list#.reset_index()#['level_0'].unique()#.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split the  new subset of the  data acquired by feature selection into train and test set and fit the logistic regression model on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the new subset of data and fit the logistic model on training data\n",
    "X_new = X.drop([33,39],1)\n",
    "y =df[57]\n",
    "X_train1,X_test1,y_train1,y_test1 = train_test_split(X_new,y,test_size = 0.3, random_state = 42)\n",
    "model1 = LogisticRegression()\n",
    "model1.fit(X_train1, y_train1)\n",
    "y_pred1 = model1.predict(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Find out the accuracy , print out the Classification report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9304851556842868\n",
      "f1_score is 0.9150442477876107\n",
      "A precision_score is 0.9349005424954792\n",
      "A recall_score is 0.8960138648180243\n",
      "A roc_auc_score is 0.9256188727075196\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Classification report and Confusion Matrix for new data\n",
    "print('Accuracy Score is',accuracy_score(y_test1,y_pred1))\n",
    "print('f1_score is',f1_score(y_test1,y_pred1))\n",
    "print('A precision_score is',precision_score(y_test1,y_pred1))\n",
    "print('A recall_score is',recall_score(y_test1,y_pred1))\n",
    "print('A roc_auc_score is',roc_auc_score(y_test1,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517 36\n",
      "60 768\n"
     ]
    }
   ],
   "source": [
    "tn,fp,fn,tp = confusion_matrix(y_test1,y_pred1).ravel()\n",
    "print(tp,fp)\n",
    "print(fn,tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. After keeping highly correlated features, there is not much change in the score. Lets apply another feature selection technique(Chi Squared test) to see whether we can increase our score. Find the optimum number of features using Chi Square and fit the logistic model on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Chi Square and fit the logistic model on train data use df dataset\n",
    "# import packages\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Code starts here\n",
    "l = [10,15,20,25,30,35,40,45,50,55]\n",
    "scores = []\n",
    "for i in l:\n",
    "    test = SelectKBest(score_func=chi2,k=i)\n",
    "    X_train2,X_test2,y_train2,y_test2 = train_test_split(X_new,y,test_size = 0.3,random_state = 42)\n",
    "    X_train2 = test.fit_transform(X_train2,y_train2)\n",
    "    X_test2 = test.transform(X_test2)\n",
    "    model2 = LogisticRegression(random_state = 101)\n",
    "    model2.fit(X_train2,y_train2)\n",
    "    scores.append(model2.score(X_test2,y_test2))\n",
    "    max_index = scores.index(max(scores))\n",
    "    best_k = l[max_index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 columns will give the best score which is 0.9304851556842868\n"
     ]
    }
   ],
   "source": [
    "print(best_k,'columns will give the best score which is',max(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SelectKBest(score_func=chi2,k=best_k)\n",
    "X_train2,X_test2,y_train2,y_test2 = train_test_split(X_new,y,test_size = 0.3,random_state = 42)\n",
    "X_train2 = test.fit_transform(X_train2,y_train2)\n",
    "X_test2 = test.transform(X_test2)\n",
    "model2 = LogisticRegression(random_state = 101)\n",
    "model2.fit(X_train2,y_train2)\n",
    "model2.score(X_test2,y_test2)\n",
    "y_pred2 = model2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3220, 55)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.429815205412069"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.coef_.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Find out the accuracy , print out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9304851556842868\n",
      "f1_score is 0.9150442477876107\n",
      "A precision_score is 0.9349005424954792\n",
      "A recall_score is 0.8960138648180243\n",
      "A roc_auc_score is 0.9256188727075196\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "# Calculate accuracy , print out the Classification report and Confusion Matrix for new data\n",
    "print('Accuracy Score is',accuracy_score(y_test2,y_pred2))\n",
    "print('f1_score is',f1_score(y_test2,y_pred2))\n",
    "print('A precision_score is',precision_score(y_test2,y_pred2))\n",
    "print('A recall_score is',recall_score(y_test2,y_pred2))\n",
    "print('A roc_auc_score is',roc_auc_score(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517 36\n",
      "60 768\n"
     ]
    }
   ],
   "source": [
    "tn,fp,fn,tp = confusion_matrix(y_test1,y_pred1).ravel()\n",
    "print(tp,fp)\n",
    "print(fn,tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Using chi squared test there is no change in the score and the optimum features that we got is 55. Now lets see if we can increase our score using another feature selection technique called Anova.Find the optimum number of features using Anova and fit the logistic model on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Chi Square and fit the logistic model on train data use df dataset\n",
    "# import packages\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Code starts here\n",
    "l1 = [15,20,25,30,35,40,45,50,55]\n",
    "scores1 = []\n",
    "for j in l1:\n",
    "    test1 = SelectKBest(score_func=f_classif,k=j)\n",
    "    X_train3,X_test3,y_train3,y_test3 = train_test_split(X_new,y,test_size = 0.3,random_state = 42)\n",
    "    X_train3 = test1.fit_transform(X_train3,y_train3)\n",
    "    X_test3 = test1.transform(X_test3)\n",
    "    model3 = LogisticRegression(random_state = 101)\n",
    "    model3.fit(X_train3,y_train3)\n",
    "    scores1.append(model3.score(X_test3,y_test3))\n",
    "    max_index1 = scores1.index(max(scores1))\n",
    "    best_k1 = l1[max_index1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 columns will give the best score which is 0.9304851556842868\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "print(best_k1,'columns will give the best score which is',max(scores1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Find out the accuracy , print out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = SelectKBest(score_func=f_classif,k=55)\n",
    "X_train3,X_test3,y_train3,y_test3 = train_test_split(X_new,y,test_size = 0.3,random_state = 42)\n",
    "X_train3 = test1.fit_transform(X_train3,y_train3)\n",
    "X_test3 = test1.transform(X_test3)\n",
    "model3 = LogisticRegression(random_state = 101)\n",
    "model3.fit(X_train3,y_train3)\n",
    "model3.score(X_test3,y_test3)\n",
    "y_pred3 = model3.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       804\n",
      "           1       0.93      0.90      0.92       577\n",
      "\n",
      "    accuracy                           0.93      1381\n",
      "   macro avg       0.93      0.93      0.93      1381\n",
      "weighted avg       0.93      0.93      0.93      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test3,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517 36\n",
      "60 768\n"
     ]
    }
   ],
   "source": [
    "tn,fp,fn,tp = confusion_matrix(y_test3,y_pred3).ravel()\n",
    "print(tp,fp)\n",
    "print(fn,tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Unfortunately Anova also couldn't give us a better score . Let's finally attempt PCA on train data and find if it helps in  giving a better model by reducing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA and fit the logistic model on train data use df dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "l = [10,15,20,25,30,35,40,45,50,55]\n",
    "scores2 = []\n",
    "for k in l:\n",
    "    X_train4,X_test4,y_train4,y_test4 = train_test_split(X_new,y,test_size = 0.3 ,random_state = 42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train4)\n",
    "    X_test_scaled = scaler.transform(X_test4)\n",
    "    model4 = LogisticRegression(random_state=101)\n",
    "    pca = PCA(n_components=k,random_state=101)\n",
    "    X_train_pca = pca.fit_transform(X_train4)\n",
    "    X_test_pca = pca.transform(X_test4)\n",
    "    model4.fit(X_train_pca,y_train)\n",
    "    scores2.append(model4.score(X_test_pca,y_test4))\n",
    "    max_index2 = scores2.index(max(scores2))\n",
    "    best_k2 = l[max_index2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 columns will give the best score which is 0.9304851556842868\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "print(best_k2,'columns will give the best score which is',max(scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.11960484e-01, 8.58199130e-02, 2.12910147e-03, 2.96754904e-05,\n",
       "       1.06745758e-05, 6.87830391e-06, 4.65670399e-06, 3.95432009e-06,\n",
       "       3.61022308e-06, 2.90573432e-06, 2.77736194e-06, 2.17434499e-06,\n",
       "       2.10771314e-06, 1.83977929e-06, 1.80194660e-06, 1.65058978e-06,\n",
       "       1.50930055e-06, 1.29044462e-06, 1.09947295e-06, 1.02682317e-06,\n",
       "       9.76142660e-07, 7.43927344e-07, 6.98523302e-07, 6.63532863e-07,\n",
       "       6.22461064e-07, 5.87250942e-07, 5.38859170e-07, 4.95142647e-07,\n",
       "       4.76571239e-07, 4.00971330e-07, 3.83134090e-07, 3.76061898e-07,\n",
       "       3.40472693e-07, 3.31998263e-07, 3.07545347e-07, 2.96605488e-07,\n",
       "       2.89080793e-07, 2.75828358e-07, 2.16646335e-07, 2.08879411e-07,\n",
       "       1.89759814e-07, 1.75280436e-07, 1.67338240e-07, 1.54105780e-07,\n",
       "       1.40050239e-07, 1.32659835e-07, 1.22546420e-07, 1.09140100e-07,\n",
       "       1.04289418e-07, 9.56760967e-08, 8.60126688e-08, 8.26121313e-08,\n",
       "       4.12677104e-08, 2.47829189e-08, 1.32346856e-08])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>y_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-225.497307</td>\n",
       "      <td>11.049887</td>\n",
       "      <td>-1.688375</td>\n",
       "      <td>-1.277932</td>\n",
       "      <td>-2.366843</td>\n",
       "      <td>1.426401</td>\n",
       "      <td>-0.009904</td>\n",
       "      <td>-0.023504</td>\n",
       "      <td>0.758738</td>\n",
       "      <td>-1.500528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023657</td>\n",
       "      <td>-0.009255</td>\n",
       "      <td>0.063458</td>\n",
       "      <td>0.028799</td>\n",
       "      <td>0.024660</td>\n",
       "      <td>-0.192432</td>\n",
       "      <td>0.018454</td>\n",
       "      <td>0.006057</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-221.350276</td>\n",
       "      <td>11.316631</td>\n",
       "      <td>0.392469</td>\n",
       "      <td>-0.624207</td>\n",
       "      <td>0.957452</td>\n",
       "      <td>-1.554628</td>\n",
       "      <td>-0.140463</td>\n",
       "      <td>-0.489924</td>\n",
       "      <td>0.970829</td>\n",
       "      <td>0.842888</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003613</td>\n",
       "      <td>-0.035677</td>\n",
       "      <td>-0.033939</td>\n",
       "      <td>-0.016204</td>\n",
       "      <td>0.107929</td>\n",
       "      <td>0.133482</td>\n",
       "      <td>-0.005490</td>\n",
       "      <td>-0.023792</td>\n",
       "      <td>-0.009280</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>225.604815</td>\n",
       "      <td>-61.219276</td>\n",
       "      <td>3.944527</td>\n",
       "      <td>-0.796715</td>\n",
       "      <td>-1.145931</td>\n",
       "      <td>0.249165</td>\n",
       "      <td>-0.063717</td>\n",
       "      <td>-0.164779</td>\n",
       "      <td>-0.273459</td>\n",
       "      <td>-0.302870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040782</td>\n",
       "      <td>-0.008576</td>\n",
       "      <td>-0.131857</td>\n",
       "      <td>0.049519</td>\n",
       "      <td>0.226423</td>\n",
       "      <td>-0.014075</td>\n",
       "      <td>-0.022712</td>\n",
       "      <td>-0.003609</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-139.529923</td>\n",
       "      <td>37.818384</td>\n",
       "      <td>-3.614960</td>\n",
       "      <td>-1.171879</td>\n",
       "      <td>-1.542043</td>\n",
       "      <td>0.335552</td>\n",
       "      <td>-0.142962</td>\n",
       "      <td>1.150897</td>\n",
       "      <td>-0.359794</td>\n",
       "      <td>-0.717291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050937</td>\n",
       "      <td>-0.023321</td>\n",
       "      <td>-0.023551</td>\n",
       "      <td>-0.251106</td>\n",
       "      <td>-0.556769</td>\n",
       "      <td>0.527773</td>\n",
       "      <td>-0.026692</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.010538</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-218.009725</td>\n",
       "      <td>49.203917</td>\n",
       "      <td>7.530543</td>\n",
       "      <td>-0.523369</td>\n",
       "      <td>10.556501</td>\n",
       "      <td>8.287837</td>\n",
       "      <td>0.488933</td>\n",
       "      <td>1.252598</td>\n",
       "      <td>-0.988779</td>\n",
       "      <td>-0.103982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>-0.106200</td>\n",
       "      <td>-0.031660</td>\n",
       "      <td>-0.144653</td>\n",
       "      <td>0.066954</td>\n",
       "      <td>-0.022573</td>\n",
       "      <td>-0.039083</td>\n",
       "      <td>-0.014511</td>\n",
       "      <td>0.004103</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1          2         3         4          5         6         7  \\\n",
       "0 -225.497307  11.049887 -1.688375 -1.277932  -2.366843  1.426401 -0.009904   \n",
       "1 -221.350276  11.316631  0.392469 -0.624207   0.957452 -1.554628 -0.140463   \n",
       "2  225.604815 -61.219276  3.944527 -0.796715  -1.145931  0.249165 -0.063717   \n",
       "3 -139.529923  37.818384 -3.614960 -1.171879  -1.542043  0.335552 -0.142962   \n",
       "4 -218.009725  49.203917  7.530543 -0.523369  10.556501  8.287837  0.488933   \n",
       "\n",
       "          8         9        10   ...           47        48        49  \\\n",
       "0 -0.023504  0.758738 -1.500528   ...    -0.023657 -0.009255  0.063458   \n",
       "1 -0.489924  0.970829  0.842888   ...    -0.003613 -0.035677 -0.033939   \n",
       "2 -0.164779 -0.273459 -0.302870   ...    -0.040782 -0.008576 -0.131857   \n",
       "3  1.150897 -0.359794 -0.717291   ...    -0.050937 -0.023321 -0.023551   \n",
       "4  1.252598 -0.988779 -0.103982   ...     0.001367 -0.106200 -0.031660   \n",
       "\n",
       "         50        51        52        53        54        55  y_train  \n",
       "0  0.028799  0.024660 -0.192432  0.018454  0.006057  0.000926      1.0  \n",
       "1 -0.016204  0.107929  0.133482 -0.005490 -0.023792 -0.009280      1.0  \n",
       "2  0.049519  0.226423 -0.014075 -0.022712 -0.003609 -0.004452      1.0  \n",
       "3 -0.251106 -0.556769  0.527773 -0.026692  0.001844  0.010538      1.0  \n",
       "4 -0.144653  0.066954 -0.022573 -0.039083 -0.014511  0.004103      1.0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_df = pd.DataFrame(data = X_train_pca ,columns = np.arange(1,56))\n",
    "pc_df['y_train'] = y_train4\n",
    "pc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,8))\n",
    "df_pca = pd.DataFrame({'var':pca.explained_variance_ratio_,'PC':np.arange(1,56)})\n",
    "sns.barplot(x='PC',y=\"var\", data=df_pca, color=\"c\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values = pca.explained_variance_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Scree plot')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(eigen_values,color='green', linewidth = 3, \n",
    "         marker='o', markerfacecolor='blue', markersize=7)\n",
    "plt.xticks(range(0,55,1))\n",
    "plt.xlabel('Components')\n",
    "plt.ylabel('Eigenvalues')\n",
    "plt.title('Scree plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Find out the accuracy , print out the Confusion Matrix.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare observed value and Predicted value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       804\n",
      "           1       0.93      0.90      0.92       577\n",
      "\n",
      "    accuracy                           0.93      1381\n",
      "   macro avg       0.93      0.93      0.93      1381\n",
      "weighted avg       0.93      0.93      0.93      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "y_pred4 = model4.predict(X_test_pca)\n",
    "print(classification_report(y_test3,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516 35\n",
      "61 769\n"
     ]
    }
   ],
   "source": [
    "tn,fp,fn,tp = confusion_matrix(y_test4,y_pred4).ravel()\n",
    "print(tp,fp)\n",
    "print(fn,tn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
